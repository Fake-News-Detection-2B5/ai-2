{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rob.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djfv-ZcRKsu6",
        "outputId": "b58562d4-7a03-4203-ae77-b03d63feb0ce"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WNXDSBh8aIy",
        "outputId": "d7af5ca6-eb0d-4c79-87d6-abd1c86b20a6"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "import torch\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    def download_dependencies(self):\n",
        "        pass\n",
        "\n",
        "    def process_text(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self):\n",
        "        pass\n",
        "\n",
        "class ROBERTA(torch.nn.Module, Model):\n",
        "    def __init__(self, text, dropout_rate=0.4):\n",
        "        super(ROBERTA, self).__init__()\n",
        "        self.text = text\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base',return_dict=False, num_labels = 4)\n",
        "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l1 = torch.nn.Linear(768, 64)\n",
        "        self.bn1 = torch.nn.LayerNorm(64)\n",
        "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
        "        self.l2 = torch.nn.Linear(64, 4)\n",
        "        \n",
        "    def download_dependencies(self):\n",
        "        try:\n",
        "            nltk.data.find('stopwords', './nltk_data')\n",
        "            nltk.data.find('wordnet', './nltk_data')\n",
        "        except LookupError:\n",
        "            nltk.download('stopwords', './nltk_data')\n",
        "            nltk.download('wordnet', './nltk_data')\n",
        "\n",
        "    def process_text(self):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        corpus = []\n",
        "\n",
        "        for i in range(len(self.text)):\n",
        "            review = re.sub('[^a-zA-Z]', ' ', self.text[i])\n",
        "            review = review.lower()\n",
        "            review = review.split()\n",
        "            review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "            review = ' '.join(review)\n",
        "            corpus.append(review)\n",
        "        self.text = corpus\n",
        "\n",
        "\n",
        "    def load_checkpoint(self,path, model):    \n",
        "        state_dict = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(state_dict['model_state_dict'])\n",
        "\n",
        "        return state_dict['valid_loss']\n",
        "\n",
        "    def predict(self):\n",
        "        labels_output = []\n",
        "        labels = ['false', 'true', 'partially false', 'other']\n",
        "        roberta_encoded_dict = self.tokenizer.encode_plus(\n",
        "                        self.text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 128,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "        roberta_encoded_dict = roberta_encoded_dict.to(device)\n",
        "        outputs = self(**roberta_encoded_dict)\n",
        "        labels_output.append(labels[outputs.argmax()])\n",
        "        return labels_output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        x = self.d1(x)\n",
        "        x = self.l1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.nn.Tanh()(x)\n",
        "        x = self.d2(x)\n",
        "        x = self.l2(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Fake News Classification')\n",
        "    parser.add_argument('text', metavar='text', type=str, nargs='+', help='Text to be classified')\n",
        "    args = parser.parse_args()\n",
        "    return ' '.join(args.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1d-HFTgb3Bg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJSxJhMSw3JS",
        "outputId": "493d7ae4-eab5-4d89-ff2a-721b2065281a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JBfl8O89Tvc",
        "outputId": "6c12fb2c-3df9-476d-b715-d4db9407302a"
      },
      "source": [
        "torch.manual_seed(17)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9DSKLpRUDPi",
        "outputId": "ed3f3346-c73b-416f-9ed8-51632b11f4f9"
      },
      "source": [
        "model = ROBERTA(\"Mark Zuckerberg is the owner of Facebook\")\n",
        "model.to(device)\n",
        "model.load_checkpoint(\"/content/drive/MyDrive/model.pkl\", model)\n",
        "model.download_dependencies()\n",
        "model.process_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to ./nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "2-SHSVceXBdF",
        "outputId": "6a9a1961-177b-4ce6-cbb1-260d2256ec1f"
      },
      "source": [
        "model.predict()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'false'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}